# Editerra RAC-CAG Coding Assistant - Complete Analysis

**Date:** November 19, 2025  
**Product Name:** Editerra RAC-CAG (Retrieval-Augmented Context-Aware Generator)  
**Purpose:** Transform RACAG from a Kairos-specific tool into a universal, commercial-ready coding assistant

---

## Executive Summary

**Editerra RAC-CAG** is a sophisticated AI-powered code intelligence engine that creates vectorized knowledge bases for complex projects. It currently lives inside the KairosAmiqo repository with several hardcoded paths and assumptions. This document analyzes its architecture and proposes a complete separation into a standalone, commercial product.

**Distribution Strategy:** Tri-platform deployment (ALL THREE):
1. **Python Package** (`pip install editerra-racag`) - Core engine + CLI
2. **VS Code Extension** (`editerra-racag`) - Full IDE integration
3. **MCP Server** (`editerra-racag-mcp`) - Claude Desktop & MCP clients

**Business Model:** Freemium with commercial licensing
**Repository:** New GitHub repo under VSLinea organization

---

## 1. Current Architecture Analysis

### 1.1 Core Pipeline

```
[Source Files] 
    ‚Üì (1) Chunking
[chunks.jsonl] 
    ‚Üì (2) Embedding (OpenAI)
[ChromaDB] 
    ‚Üì (3) Query + Rerank
[Context Results]
```

### 1.2 Component Breakdown

| Component | Purpose | Files | Dependencies |
|-----------|---------|-------|--------------|
| **Chunking** | Parse code/docs into semantic chunks | `chunking/*.py` | tree-sitter, pathlib |
| **Embedding** | Convert chunks to vectors | `embedding/*.py` | OpenAI API, ChromaDB |
| **Storage** | Persist vectors | `db/` | ChromaDB (SQLite) |
| **Retrieval** | Find similar chunks | `retrieval/*.py`, `query/*.py` | ChromaDB |
| **Reranking** | Hybrid scoring (cosine + LLM) | `reranker/*.py` | OpenAI (gpt-4.1-mini) |
| **Context Assembly** | Format results for LLMs | `context/*.py` | tiktoken |
| **API Server** | HTTP interface | `api/*.py`, `server/*.py` | FastAPI, uvicorn |
| **File Watcher** | Real-time updates | `watcher/*.py` | watchdog |

---

## 2. Project-Specific vs. Reusable Code

### 2.1 HARDCODED (Needs Fixing)

| Location | Issue | Impact |
|----------|-------|--------|
| `watcher/file_watcher.py:12` | `WATCH_DIRECTORY = "/Users/lyra/KairosMain/ios/KairosAmiqo"` | üî¥ Critical |
| `query/query_engine.py:50-51` | `CHROMA_PATH = "racag/db/chroma_store"`<br>`COLL_NAME = "kairos_chunks"` | üü° Medium |
| `embedding/embed_all.py:9-11` | Hardcoded paths and collection names | üü° Medium |
| `chunking/run_chunkers.py:151` | `Path("racag/output")` | üü° Medium |
| `chunking/code_chunker.py:8` | `SWIFT_LANGUAGE_LIB = "racag/tree_sitter_languages/build/my-languages.so"` | üü° Medium |
| Collection names | "kairos_chunks" used everywhere | üü¢ Low |

### 2.2 REUSABLE (Already Good)

‚úÖ **Chunking Logic**: Generic parsers for Swift, Python, JS, Markdown, JSON  
‚úÖ **Embedding Pipeline**: Language-agnostic OpenAI integration  
‚úÖ **Query Engine**: No project assumptions  
‚úÖ **Reranking Algorithm**: Portable hybrid scoring  
‚úÖ **Context Assembly**: Token-aware formatting  
‚úÖ **API Structure**: Clean FastAPI implementation  

### 2.3 LANGUAGE SUPPORT

**Currently Supported:**
- Swift (tree-sitter based)
- Python (tree-sitter ready)
- JavaScript/TypeScript (tree-sitter ready)
- Markdown (regex-based heading detection)
- JSON (key extraction)
- YAML, Shell scripts (basic text chunking)

**Missing (but easy to add):**
- Java, C++, C#, Go, Rust, Ruby, PHP
- Tree-sitter grammars exist for all of these

---

## 3. External Dependencies

### 3.1 Python Requirements
```
chromadb>=0.4.0          # Vector DB
openai>=1.0.0            # Embeddings + reranking
tiktoken>=0.5.0          # Token counting
fastapi>=0.104.0         # API server
uvicorn>=0.24.0          # ASGI server
pydantic>=2.0.0          # Data validation
python-dotenv>=1.0.0     # Config management
PyYAML>=6.0              # Config files
tree-sitter>=0.20.0      # Code parsing
watchdog>=3.0.0          # File watching
matplotlib>=3.7.0        # Diagnostics
opentelemetry-api>=1.20.0 # Telemetry
```

### 3.2 Tree-sitter Binary
- **Current:** Custom-built `.so` file for Swift only
- **Solution:** Package pre-built grammars or build on-demand
- **Distribution:** Include common languages, lazy-load others

### 3.3 LLM Provider Support (Multi-Provider Architecture)

**Current:** OpenAI only  
**Target:** Support ANY LLM via unified API abstraction

**Supported Providers:**
1. **OpenAI** (default)
   - `text-embedding-3-large` / `text-embedding-3-small` (embeddings)
   - `gpt-4.1-mini` / `gpt-4o-mini` (reranking)
   - Cost: ~$0.01-0.05 per 1000 chunks

2. **Anthropic Claude**
   - `voyage-3` embeddings (via Voyage AI partnership)
   - `claude-3-haiku` (reranking)
   
3. **Azure OpenAI**
   - Same models, different endpoint
   - Enterprise SSO integration

4. **AWS Bedrock**
   - Titan embeddings
   - Claude via Bedrock

5. **Google Vertex AI**
   - `text-embedding-gecko` / `text-embedding-004`
   - `gemini-1.5-flash` (reranking)

6. **Ollama (Local)**
   - `nomic-embed-text` (embeddings)
   - `llama3.1:8b` / `mistral` (reranking)
   - **Cost: $0** (fully local)

7. **Cohere**
   - `embed-english-v3.0` (embeddings)
   - `command-r` (reranking)

8. **HuggingFace**
   - `sentence-transformers` (local embeddings)
   - Any hosted inference endpoint

**Architecture Pattern:**
```python
# Unified LLM abstraction
class LLMProvider(ABC):
    @abstractmethod
    def embed(self, texts: List[str]) -> List[List[float]]:
        pass
    
    @abstractmethod
    def rerank(self, query: str, candidates: List[str]) -> List[float]:
        pass

# Implementations
class OpenAIProvider(LLMProvider): ...
class AnthropicProvider(LLMProvider): ...
class OllamaProvider(LLMProvider): ...
```

---

## 4. Integration Points

### 4.1 Current Interfaces

| Interface | File | Purpose |
|-----------|------|---------|
| **CLI** | `test_cli.py`, `query.py` | Direct queries |
| **MCP Adapter** | `mcp_adapter.py` | Claude Desktop integration |
| **FastAPI Server** | `api/server.py` | HTTP endpoint |
| **Copilot Adapter** | `api/copilot_adapter.py` | VS Code Copilot |

### 4.2 Data Flow

**Input Formats:**
- Natural language query string
- Optional: `top_k` parameter (default: 3-5)

**Output Format:**
```json
{
  "query": "Where is DashboardView?",
  "status": "success",
  "count": 3,
  "results": [
    {
      "id": "file::hash",
      "score_hybrid": 0.91,
      "score_cosine": 0.83,
      "score_llm": 0.95,
      "file": "/path/to/file.swift",
      "lang": "swift",
      "lines": "210-268",
      "text": "struct DashboardView..."
    }
  ]
}
```

---

## 5. Proposed Standalone Architecture

### 5.1 Package Name
**Suggestion:** `CodeRACAG` or `RACAG` or `VectorCodeIndex`

**Reasoning:**
- "RACAG" is unique and searchable
- "CodeRACAG" is more descriptive
- Avoid generic names like "codebase-ai" (taken)

**Final Recommendation:** `racag` (lowercase, Python convention)

### 5.2 Repository Structure

```
racag/                          # New standalone repo
‚îú‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ LICENSE
‚îú‚îÄ‚îÄ setup.py / pyproject.toml   # Python packaging
‚îú‚îÄ‚îÄ .gitignore
‚îÇ
‚îú‚îÄ‚îÄ racag/                      # Python package
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ __main__.py             # CLI entry: python -m racag
‚îÇ   ‚îú‚îÄ‚îÄ config.py               # Dynamic configuration
‚îÇ   ‚îú‚îÄ‚îÄ chunking/
‚îÇ   ‚îú‚îÄ‚îÄ embedding/
‚îÇ   ‚îú‚îÄ‚îÄ retrieval/
‚îÇ   ‚îú‚îÄ‚îÄ reranker/
‚îÇ   ‚îú‚îÄ‚îÄ query/
‚îÇ   ‚îú‚îÄ‚îÄ context/
‚îÇ   ‚îú‚îÄ‚îÄ api/
‚îÇ   ‚îú‚îÄ‚îÄ server/
‚îÇ   ‚îú‚îÄ‚îÄ watcher/
‚îÇ   ‚îú‚îÄ‚îÄ tree_sitter/            # Grammar management
‚îÇ   ‚îî‚îÄ‚îÄ utils/
‚îÇ
‚îú‚îÄ‚îÄ extensions/
‚îÇ   ‚îî‚îÄ‚îÄ vscode/                 # VS Code extension
‚îÇ       ‚îú‚îÄ‚îÄ package.json
‚îÇ       ‚îú‚îÄ‚îÄ tsconfig.json
‚îÇ       ‚îú‚îÄ‚îÄ src/
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ extension.ts
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ racagClient.ts
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ statusBar.ts
‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ commands/
‚îÇ       ‚îî‚îÄ‚îÄ README.md
‚îÇ
‚îú‚îÄ‚îÄ mcp/                        # MCP Server
‚îÇ   ‚îú‚îÄ‚îÄ server.py
‚îÇ   ‚îú‚îÄ‚îÄ README.md
‚îÇ   ‚îî‚îÄ‚îÄ config.json
‚îÇ
‚îú‚îÄ‚îÄ docs/
‚îÇ   ‚îú‚îÄ‚îÄ installation.md
‚îÇ   ‚îú‚îÄ‚îÄ configuration.md
‚îÇ   ‚îú‚îÄ‚îÄ api-reference.md
‚îÇ   ‚îî‚îÄ‚îÄ architecture.md
‚îÇ
‚îî‚îÄ‚îÄ tests/
    ‚îú‚îÄ‚îÄ test_chunking.py
    ‚îú‚îÄ‚îÄ test_embedding.py
    ‚îî‚îÄ‚îÄ test_query.py
```

### 5.3 Configuration System

**Per-Project Config:** `.racag.yaml` in workspace root

```yaml
# .racag.yaml
version: 1

# Project metadata
project_name: "MyProject"
collection_name: "myproject_chunks"  # Auto-generated if not set

# Paths (relative to workspace root)
db_path: ".racag/db"
output_path: ".racag/output"
cache_path: ".racag/cache"

# Indexing settings
watch_enabled: true
watch_paths:
  - "src"
  - "lib"
  - "docs"

exclude_dirs:
  - "node_modules"
  - ".git"
  - "build"
  - "dist"
  - "__pycache__"

include_extensions:
  - ".py"
  - ".ts"
  - ".js"
  - ".swift"
  - ".md"
  - ".json"

# LLM Provider settings
llm_provider: "openai"  # openai, anthropic, azure, bedrock, vertex, ollama, cohere, huggingface

# OpenAI settings
openai:
  api_key: "${OPENAI_API_KEY}"
  embedding_model: "text-embedding-3-large"
  rerank_model: "gpt-4o-mini"
  embedding_dimensions: 1536

# Anthropic settings
anthropic:
  api_key: "${ANTHROPIC_API_KEY}"
  embedding_model: "voyage-3"
  rerank_model: "claude-3-haiku-20240307"

# Azure OpenAI settings
azure:
  api_key: "${AZURE_OPENAI_API_KEY}"
  endpoint: "${AZURE_OPENAI_ENDPOINT}"
  deployment_name: "gpt-4"
  api_version: "2024-02-15-preview"

# Ollama settings (local, free)
ollama:
  base_url: "http://localhost:11434"
  embedding_model: "nomic-embed-text"
  rerank_model: "llama3.1:8b"

# Google Vertex AI settings
vertex:
  project_id: "${GCP_PROJECT_ID}"
  location: "us-central1"
  embedding_model: "text-embedding-004"
  rerank_model: "gemini-1.5-flash"

# Cohere settings
cohere:
  api_key: "${COHERE_API_KEY}"
  embedding_model: "embed-english-v3.0"
  rerank_model: "command-r"

# Embedding settings
embedding_batch_size: 32
embedding_cache: true

# Query settings
retrieve_k: 40
final_k: 5

# API settings (optional)
api_enabled: false
api_port: 8009
api_host: "127.0.0.1"
```

**Global Config:** `~/.racag/config.yaml`

```yaml
# Global RACAG settings
openai_api_key: "${OPENAI_API_KEY}"  # Or directly set
default_embedding_model: "text-embedding-3-large"
telemetry_enabled: false
cache_embeddings: true
```

### 5.4 Workspace Detection

**Auto-detection Strategy:**

1. **Check for `.racag.yaml`** ‚Üí Use explicit config
2. **Detect project type:**
   - `package.json` ‚Üí Node.js/TypeScript project
   - `requirements.txt` or `pyproject.toml` ‚Üí Python project
   - `*.xcodeproj` ‚Üí iOS/Swift project
   - `pom.xml` / `build.gradle` ‚Üí Java project
   - `Cargo.toml` ‚Üí Rust project
   - `go.mod` ‚Üí Go project
3. **Generate default config** based on project type
4. **Initialize database** at `.racag/db/`

### 5.5 Dynamic Path Resolution

**Replace all hardcoded paths with:**

```python
# racag/config.py
from pathlib import Path
import os
import yaml

class RacagConfig:
    def __init__(self, workspace_root: Path = None):
        self.workspace_root = workspace_root or Path.cwd()
        self.config_file = self.workspace_root / ".racag.yaml"
        self.config = self._load_config()
        
    def _load_config(self):
        # Load .racag.yaml or create default
        if self.config_file.exists():
            with open(self.config_file) as f:
                return yaml.safe_load(f)
        return self._default_config()
    
    def _default_config(self):
        return {
            "project_name": self.workspace_root.name,
            "collection_name": f"{self.workspace_root.name}_chunks",
            "db_path": ".racag/db",
            "output_path": ".racag/output",
            # ... etc
        }
    
    @property
    def db_path(self) -> Path:
        return self.workspace_root / self.config["db_path"]
    
    @property
    def output_path(self) -> Path:
        return self.workspace_root / self.config["output_path"]
    
    @property
    def collection_name(self) -> str:
        return self.config["collection_name"]
```

**Usage in modules:**

```python
# Before:
CHROMA_PATH = "racag/db/chroma_store"
COLL_NAME = "kairos_chunks"

# After:
from racag.config import get_config
config = get_config()
CHROMA_PATH = str(config.db_path)
COLL_NAME = config.collection_name
```

---

## 6. Distribution Strategies

### 6.1 VS Code Extension (Primary)

**Package:** `racag-vscode` on VS Code Marketplace

**Features:**
- üîç **Index Workspace**: Command to chunk + embed entire project
- üìä **Status Bar**: Show indexing progress, DB stats
- ü§ñ **Copilot Integration**: Auto-inject RACAG context into prompts
- üîÑ **Auto-Watch**: Real-time incremental updates
- ‚öôÔ∏è **Settings UI**: Configure via VS Code settings
- üìù **Commands:**
  - `RACAG: Index Workspace`
  - `RACAG: Rebuild Index`
  - `RACAG: Query Codebase`
  - `RACAG: Show Stats`
  - `RACAG: Configure`

**Backend Communication:**
- Extension spawns Python process (bundled or system Python)
- Communicates via stdio or local HTTP server
- Falls back to MCP if available

**Installation:**
1. Install from VS Code Marketplace
2. Extension auto-detects Python 3.11+
3. Prompts to install Python package: `pip install racag`
4. Prompts for OpenAI API key
5. Activates on workspace open

### 6.2 MCP Server (Secondary)

**Package:** `racag-mcp` via npm or standalone

**Purpose:**
- Claude Desktop integration
- Cline/Continue.dev integration
- Any MCP-compatible client

**Tools Exposed:**
```json
{
  "tools": [
    {
      "name": "racag_query",
      "description": "Retrieve relevant code context from project",
      "parameters": {
        "query": "string",
        "top_k": "number (optional)"
      }
    },
    {
      "name": "racag_index",
      "description": "Index or re-index the workspace"
    },
    {
      "name": "racag_stats",
      "description": "Get index statistics"
    }
  ]
}
```

**Configuration:**
```json
// Claude Desktop config
{
  "mcpServers": {
    "racag": {
      "command": "racag-mcp",
      "args": ["--workspace", "/path/to/project"],
      "env": {
        "OPENAI_API_KEY": "sk-..."
      }
    }
  }
}
```

### 6.3 Python Package (Core)

**Package:** `racag` on PyPI

**Installation:**
```bash
pip install racag
# or
pipx install racag  # For CLI tool
```

**CLI Usage:**
```bash
# Initialize in current directory
racag init

# Index the workspace
racag index

# Query
racag query "Where is the authentication logic?"

# Start API server
racag serve --port 8009

# Watch mode
racag watch
```

**Python API:**
```python
from racag import RacagEngine

engine = RacagEngine(workspace_root="/path/to/project")
engine.index()

results = engine.query("Find all API routes")
for result in results:
    print(f"{result.file}:{result.lines} - {result.score}")
```

---

## 7. Implementation Roadmap

### Phase 1: Core Refactoring (2-3 days)
- [ ] Extract configuration system
- [ ] Replace hardcoded paths with dynamic resolution
- [ ] Create `RacagConfig` class
- [ ] Add workspace detection logic
- [ ] Make collection names dynamic
- [ ] Remove Kairos-specific references

### Phase 2: Python Package (2-3 days)
- [ ] Create `setup.py` / `pyproject.toml`
- [ ] CLI interface with Click or Typer
- [ ] Proper logging configuration
- [ ] Tree-sitter grammar management
- [ ] Package tree-sitter binaries (or build script)
- [ ] Write comprehensive tests
- [ ] Documentation

### Phase 3: VS Code Extension (3-4 days)
- [ ] TypeScript project setup
- [ ] Extension activation logic
- [ ] Python backend communication
- [ ] Status bar UI
- [ ] Commands palette integration
- [ ] Settings schema
- [ ] Copilot integration (context provider)
- [ ] Progress notifications
- [ ] Error handling

### Phase 4: MCP Server (1-2 days)
- [ ] Standalone MCP server script
- [ ] Tool definitions
- [ ] Configuration file
- [ ] Testing with Claude Desktop
- [ ] Documentation

### Phase 5: Publishing (1-2 days)
- [ ] Publish to PyPI
- [ ] Publish to VS Code Marketplace
- [ ] Publish MCP server (npm or GitHub releases)
- [ ] Create landing page / documentation site
- [ ] Demo video
- [ ] Marketing materials

### Phase 6: Multi-Language Support (ongoing)
- [ ] Add Python tree-sitter grammar
- [ ] Add JavaScript/TypeScript grammar
- [ ] Add Java grammar
- [ ] Add C++ grammar
- [ ] Add Go grammar
- [ ] Auto-download grammars on demand

---

## 8. Key Challenges & Solutions

### Challenge 1: Tree-sitter Binary Distribution
**Problem:** Custom `.so` files are platform-specific  
**Solution:**
- Package pre-built binaries for common platforms (macOS/Linux/Windows)
- Include build script for unsupported platforms
- Lazy-load grammars (download on first use)
- Fall back to regex-based chunking if tree-sitter fails

### Challenge 2: OpenAI API Key Management
**Problem:** Users need API key to use  
**Solution:**
- Prompt on first use
- Store in VS Code settings (encrypted)
- Support `.env` files
- Clear setup documentation
- Offer alternative embedding models (local models via Ollama)

### Challenge 3: Large Codebases
**Problem:** Embedding 100K+ files is slow/expensive  
**Solution:**
- Incremental indexing (only changed files)
- Smart exclusions (auto-detect build dirs)
- Progress indicators
- Cost estimation before indexing
- Caching embeddings

### Challenge 4: Cross-Platform Compatibility
**Problem:** Different path conventions, Python versions  
**Solution:**
- Use `pathlib` everywhere
- Test on macOS, Linux, Windows
- Support Python 3.9+ (not just 3.11+)
- Bundle dependencies with extension

### Challenge 5: VS Code Extension Python Communication
**Problem:** Extension needs to spawn Python process  
**Solution:**
- Try system Python first
- Allow user to specify Python path
- Bundle Python distribution (optional)
- Use language server protocol (LSP) pattern
- Fall back to HTTP if stdio fails

---

## 9. Naming & Branding

### Final Decision: Editerra RAC-CAG

**Full Name:** Editerra RAC-CAG Coding Assistant  
**Short Name:** Editerra RAC-CAG  
**Package Names:**
- Python: `editerra-racag`
- VS Code: `editerra.racag` or `editerra-racag`
- MCP: `editerra-racag-mcp`

**Branding:**
- ‚úÖ "Editerra" - Professional, memorable, domain available
- ‚úÖ "RAC-CAG" - Keeps the technical acronym
- ‚úÖ "Coding Assistant" - Clear value proposition
- ‚úÖ Unique and brandable for commercial use

**Tagline:** "AI-Powered Code Intelligence for Complex Projects"  
**Sub-tagline:** "Works with ANY LLM. Own your data. Scale your codebase."

---

## 10. Competitive Analysis

### Existing Solutions:

| Tool | Type | Strengths | Weaknesses |
|------|------|-----------|------------|
| **GitHub Copilot** | AI Assistant | Deep IDE integration | No custom codebase context |
| **Cursor** | AI Editor | Full codebase awareness | Separate editor |
| **Continue.dev** | Extension | Open source, customizable | Complex setup |
| **Codeium** | AI Assistant | Fast, free | Limited context depth |
| **Sourcegraph** | Code search | Powerful search | No AI/embedding layer |
| **Bloop** | Code search | Natural language search | Limited IDE integration |

**RACAG's Differentiators:**
1. ‚úÖ **Hybrid scoring** (cosine + LLM reranking)
2. ‚úÖ **Works with any AI tool** (Copilot, Claude, ChatGPT)
3. ‚úÖ **Fully local database** (privacy-first)
4. ‚úÖ **Incremental updates** (real-time file watching)
5. ‚úÖ **Language-agnostic** (tree-sitter based)
6. ‚úÖ **Open source** (MIT license?)

---

## 11. Licensing & Monetization Strategy

### Recommended License: **Business Source License (BSL) 1.1**

**Why BSL instead of MIT/Apache/GPL?**

| License | Commercial Use | Source Available | Monetization-Friendly | Converts to Open Source |
|---------|----------------|------------------|----------------------|------------------------|
| MIT | ‚úÖ Unlimited | ‚úÖ | ‚ùå No protection | ‚ùå |
| Apache 2.0 | ‚úÖ Unlimited | ‚úÖ | ‚ùå No protection | ‚ùå |
| GPL v3 | ‚ö†Ô∏è Copyleft required | ‚úÖ | ‚ùå Forces derivative open source | ‚ùå |
| AGPL v3 | ‚ö†Ô∏è Network copyleft | ‚úÖ | ‚ùå Even SaaS must be open | ‚ùå |
| **BSL 1.1** | ‚ö†Ô∏è **Limited (max 3 years)** | ‚úÖ | ‚úÖ **Protects commercial use** | ‚úÖ **Auto-converts to Apache 2.0** |

**Business Source License (BSL) Structure:**
- ‚úÖ **Source code is public** (not proprietary)
- ‚úÖ **Free for individuals & small teams** (< 10 developers)
- ‚úÖ **Paid license for enterprises** (‚â• 10 developers or revenue > $X)
- ‚úÖ **Automatically converts to Apache 2.0** after 3 years
- ‚úÖ **Used by:** HashiCorp, Cockroach Labs, MariaDB, Sentry

**Alternative Option: Dual Licensing**
- **AGPLv3** for open source use (forces derivatives to be open)
- **Commercial License** for companies wanting proprietary integration
- Used by: MySQL, MongoDB, Qt

**Recommended for Editerra RAC-CAG:** 
‚Üí **Business Source License (BSL 1.1)** with conversion to Apache 2.0 after 3 years

### Monetization Tiers

**Free Tier (BSL-permitted):**
- ‚úÖ Individual developers
- ‚úÖ Small teams (< 10 developers)
- ‚úÖ Open source projects
- ‚úÖ Educational use
- ‚úÖ Self-hosted deployment
- ‚úÖ Personal API key usage
- ‚ö†Ô∏è Community support only

**Pro Tier ($19/user/month):**
- ‚úÖ Teams (10-100 developers)
- ‚úÖ Priority support
- ‚úÖ Advanced features:
  - Private LLM integration (Azure OpenAI, AWS Bedrock)
  - Team context sharing
  - Advanced analytics dashboard
  - Custom chunking strategies
  - API access
- ‚úÖ Quarterly feature updates

**Enterprise Tier (Custom pricing):**
- ‚úÖ Unlimited developers
- ‚úÖ Commercial license (no BSL restrictions)
- ‚úÖ White-label deployment
- ‚úÖ On-premise installation
- ‚úÖ Custom integrations
- ‚úÖ SLA guarantees
- ‚úÖ Dedicated support
- ‚úÖ Training & onboarding

**Cloud-Hosted Option (+$9/user/month):**
- ‚úÖ No local setup required
- ‚úÖ Managed infrastructure
- ‚úÖ Included API credits
- ‚úÖ Automatic updates
- ‚úÖ Team collaboration features

### Revenue Projections

**Conservative (Year 1):**
- 10,000 free users
- 100 Pro users: $19 √ó 100 √ó 12 = $22,800/year
- 5 Enterprise deals: ~$50,000/year
- **Total: ~$72,800/year**

**Moderate (Year 2):**
- 50,000 free users
- 500 Pro users: $19 √ó 500 √ó 12 = $114,000/year
- 20 Enterprise deals: ~$300,000/year
- **Total: ~$414,000/year**

**Optimistic (Year 3):**
- 200,000 free users
- 2,000 Pro users: $19 √ó 2,000 √ó 12 = $456,000/year
- 50 Enterprise deals: ~$1,000,000/year
- **Total: ~$1,456,000/year**

---

## 12. Next Steps

### Immediate Actions (This Week):
1. ‚úÖ **Complete this analysis** ‚Üê You are here
2. **Create new repository:** `racag` or `CodeRACAG`
3. **Copy RACAG code** from KairosAmiqo
4. **Refactor configuration system**
5. **Remove hardcoded paths**
6. **Write setup.py**
7. **Create CLI interface**

### Next Week:
8. **Test on multiple projects** (Python, TypeScript, Go)
9. **Start VS Code extension**
10. **Document API**

### Following Weeks:
11. **Polish VS Code extension**
12. **Create MCP server**
13. **Write comprehensive docs**
14. **Publish alpha version**

---

## 13. Risk Assessment

| Risk | Probability | Impact | Mitigation |
|------|-------------|--------|------------|
| OpenAI cost too high for users | Medium | High | Support local models, optimize chunking |
| Tree-sitter complexity | Medium | Medium | Fall back to regex, pre-build binaries |
| Low adoption | Medium | Medium | Good docs, demo videos, marketing |
| Performance issues (large repos) | Low | High | Incremental indexing, smart exclusions |
| Python version conflicts | Low | Medium | Support wide range (3.9-3.12) |
| Security concerns (API keys) | Low | High | Encrypted storage, clear security docs |

---

## 14. Success Metrics

**Launch Goals (3 months):**
- üì¶ Published on PyPI
- üì¶ Published on VS Code Marketplace
- üë• 100+ extension installs
- ‚≠ê 50+ GitHub stars
- üìù Complete documentation
- üß™ 80%+ test coverage

**Growth Goals (6 months):**
- üë• 1,000+ extension installs
- ‚≠ê 500+ GitHub stars
- ü§ù 5+ external contributors
- üìä 10+ blog posts / tutorials
- üéØ Featured in VS Code extension roundup

**Long-term Goals (1 year):**
- üë• 10,000+ extension installs
- ‚≠ê 2,000+ GitHub stars
- üè¢ Enterprise customers
- üí∞ Sustainable paid tier
- üåç Multi-language community

---

## 15. Conclusion

RACAG is a powerful, well-architected code intelligence engine that's 85% ready to become a standalone product. The main work is:

1. **Decoupling from Kairos** (configuration system)
2. **Packaging** (Python + VS Code extension)
3. **Documentation** (comprehensive setup guides)
4. **Testing** (multi-language, multi-platform)

**Timeline Estimate:** 2-3 weeks for MVP (Python package + basic extension)  
**Timeline Estimate:** 4-6 weeks for v1.0 (full-featured extension + MCP)

**Effort Level:** Medium (mostly packaging/refactoring, core logic is solid)

**Recommended First Step:** Create separate repo, refactor config, publish Python package to PyPI as `racag` alpha.

---

**Questions to Decide:**

1. ‚ùì **Name:** Keep "RACAG" or rebrand?
2. ‚ùì **License:** MIT? Apache 2.0? GPL?
3. ‚ùì **Primary focus:** VS Code extension or Python package first?
4. ‚ùì **Repository host:** GitHub? GitLab?
5. ‚ùì **Versioning:** Start at 0.1.0 or 1.0.0-alpha?
6. ‚ùì **Tree-sitter:** Bundle binaries or build on install?
7. ‚ùì **Local models:** Support Ollama/LlamaIndex now or later?

**Your Turn:** Which aspects would you like me to start implementing?
